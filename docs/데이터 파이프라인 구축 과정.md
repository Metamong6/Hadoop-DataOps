# ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ê³¼ì •

# ëª©í‘œ

> ì›ë³¸ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ì „ì²˜ë¦¬í•´ì„œ ëª¨ë¸ì´ í•™ìŠµí•˜ê¸° ì§ì „ì˜ ë°ì´í„° ìƒíƒœê¹Œì§€ ë§Œë“  í›„ feature storeì— ì €ì¥í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ Hadoop í´ëŸ¬ìŠ¤í„°ë¡œ êµ¬ì¶•
> 
- íŒŒì´í”„ë¼ì¸ í”Œë¡œìš°
ë°ì´í„° ì†ŒìŠ¤ â†’ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ â†’ ë°ì´í„° ë¶„ì„/ì²˜ë¦¬ â†’ feature store
- íŒŒì´í”„ë¼ì¸ ìš”ì†Œ
    1. ë°ì´í„° ì†ŒìŠ¤ : NYC taxi data 
    2. ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ : HDFS (Hadoop)
    3. ë°ì´í„° ì²˜ë¦¬ : yarn on spark (pyspark)
    4. feature store : AMAZON S3


# í´ëŸ¬ìŠ¤í„° ì•„í‚¤í…ì³

- í•˜ë‘¡ í´ëŸ¬ìŠ¤í„° ì•„í‚¤í…ì³
    
    ![Untitled (6)](https://user-images.githubusercontent.com/102719063/203525067-bbf5ebdb-a630-4c10-96ae-5fa5d4249e68.png)
    
    - MasterNode # 1,2
        - **Namenode** 
          - hdfsì— ìˆëŠ” ë°ì´í„°ë¥¼ datanodeì— ë¶„ì‚°ì‹œí‚¤ê³  ê´€ë¦¬í•˜ëŠ” ê¸°ëŠ¥ì„ ë‹´ë‹¹
          - slaveì— í•´ë‹¹í•˜ëŠ” datanodeì—ê²Œ I/Oë¥¼ ë‹´ë‹¹í•˜ê³  datanodeì˜ ì´ìƒìœ ë¬´ë¥¼ í™•ì¸
        - **Journalnode**
          - Namnodeì˜ edits ì •ë³´ë¥¼ ì €ì¥í•˜ê³  ê³µìœ í•˜ëŠ” ê¸°ëŠ¥ì„ ìˆ˜í–‰
        - **Failover Controller**
          - active Namenodeì— ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ê²½ìš° í•´ë‹¹ ë…¸ë“œë¥¼ ë‚´ë¦¬ê³  standby ìƒíƒœì— ìˆëŠ” ë‹¤ë¥¸ Namenodeë¥¼ activeë¡œ ì „í™˜
        - **ResourceManager (Yarn)**
          - ì–´ëŠ ì‘ì—…ì— ìì›ì„ í• ë‹¹í• ì§€ ê²°ì •í•´ì„œ clusterì˜ í™œìš©ì„ ìµœì í™”
          - ì‘ì—…ì— ëŒ€í•œ ì²˜ë¦¬ ìš”ì²­ì„ ë°›ìœ¼ë©´, Requestë“¤ì˜ ì¼ë¶€ë¥¼ í•´ë‹¹í•˜ëŠ” NodeManagerë¡œ ì „ë‹¬
        - **Zookeeper**
          - zookeeper ì„¤ì¹˜ ë‚´ìš©ì—ì„œ ì—­í•  ì„¤ëª…
        
    - Utility Node # 1
        - **Datanode**
          - ë°ì´í„°ë…¸ë“œëŠ” í´ë¼ì´ì–¸íŠ¸ë‚˜ ë„¤ì„ë…¸ë“œì˜ ìš”ì²­ì´ ìˆì„ ë•Œ ë¸”ë¡ì„ ì €ì¥í•˜ê³  íƒìƒ‰í•˜ë©°, ì €ì¥í•˜ê³  ìˆëŠ” ë¸”ë¡ì˜ ëª©ë¡ì„ ì£¼ê¸°ì ìœ¼ë¡œ ë„¤ì„ë…¸ë“œì— ë³´ê³ 
        - **NodeManager (Yarn)**
          - í´ëŸ¬ìŠ¤í„°ì— ì†í•œÂ ë…¸ë“œë“¤ì—ì„œ ì»¨í…Œì´ë„ˆë¥¼ ì‹¤í–‰í•˜ê³ , ê° ì»¨í…Œì´ë„ˆì˜ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©ëŸ‰ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ê·¸ê²ƒì˜ ìƒíƒœë¥¼ ResourceManagerì— ë³´ê³ í•˜ëŠ” ì—­í• ì„ ë‹´ë‹¹
        - **Zookeeper**
        - **Hiveserver2, Hive metastore**
          - Hive ì„¤ì¹˜ ë‚´ìš©ì—ì„œ ì—­í•  ì„¤ëª…
        
    - Data Node # 1,2
        - **Datanode**
        - **Nodemanager (Yarn)**

# í•˜ë‘¡ í´ëŸ¬ìŠ¤í„° êµ¬ì¶• ê³¼ì •

## 1. AWS EC2 ì¸ìŠ¤í„´ìŠ¤ ë°°í¬

- í•˜ë‘¡ í´ëŸ¬ìŠ¤í„° êµ¬ì„±ì„ ìœ„í•´ EC2 ì¸ìŠ¤í„´ìŠ¤ 5ëŒ€ ë°°í¬
- ì‹¤ì§ˆì ìœ¼ë¡œ dataê°€ ì €ì¥ë˜ëŠ” **data node**ëŠ” ìŠ¤í† ë¦¬ì§€ ìš©ëŸ‰ì„ 50GBë¡œ ë†’ê²Œ ì„¤ì •
- sparkì™€ hiveë¥¼ ì‹¤í–‰ì‹œí‚¤ëŠ” **util node**ëŠ” ì›í™œí•œ ì‘ì—… ì²˜ë¦¬ë¥¼ ìœ„í•´ vCPU ìˆ˜ë¥¼ 4ê°œë¡œ ì„¤ì • (ì¸ìŠ¤í„´ìŠ¤ ìœ í˜• - t3a.xlarge)

|  | Master node # 1,2 | Util node # 1 | Data node # 1,2 |
| --- | --- | --- | --- |
| ì¸ìŠ¤í„´ìŠ¤ ìœ í˜• | t3a.medium | t3a.xlarge | t3a.medium |
| ìŠ¤í† ë¦¬ì§€ | 30GB | 50GB | 50GB |
| vCPU ìˆ˜ | 2 | 4 | 2 |
| OS | Ubuntu 20.04 | Ubuntu 20.04 | Ubuntu 20.04 |

## 2. SSH ë° í˜¸ìŠ¤íŠ¸ ì´ë¦„ ì„¤ì • - issue

- 5ê°œì˜ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ í•˜ë‘¡ì´ ë™ì‘í•˜ë ¤ë©´ ì¸ìŠ¤í„´ìŠ¤ë¼ë¦¬ ë°ì´í„°ë¥¼ ì£¼ê³  ë°›ì„ ìˆ˜ ìˆë„ë¡ ì—°ê²°í•´ì•¼í•¨.

- ì„¤ì • ìˆœì„œ
    1. ì‘ì—…ì„ ì§„í–‰í•˜ëŠ” PC(Mac)ì—ì„œ SSH ì„¤ì • í›„ ì¸ìŠ¤í„´ìŠ¤ ë°°í¬ ì‹œ ë‹¤ìš´ë¡œë“œ ë°›ì€ pem íŒŒì¼ì„ ì´ìš©í•˜ì—¬ ë°°í¬í•œ ì¸ìŠ¤í„´ìŠ¤ì— ì ‘ì†
    2. ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ì˜ /etc/hosts íŒŒì¼ì— ì•„ë˜ ì´ë¯¸ì§€ì²˜ëŸ¼ ê° ì¸ìŠ¤í„´ìŠ¤ì˜ í˜¸ìŠ¤íŠ¸ ì´ë¦„ê³¼ private IP ì¶”ê°€ 
    - ì¸ìŠ¤í„´ìŠ¤ë¼ë¦¬ SSH í†µì‹ ì´ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •í•˜ê³  ê° ì„œë²„ì˜ í˜¸ìŠ¤íŠ¸ ì´ë¦„ì„ ì„¤ì •
        
        ![Untitled (5)](https://user-images.githubusercontent.com/102719063/203524933-4e973636-a8e2-4267-b171-bab594a55ec6.png)
        
    
    3. master01ì—ì„œ ssh-keygenìœ¼ë¡œ ìƒì„±ëœ í‚¤ë¥¼ ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ì— ë°°í¬
    - ~/.ssh/authorized_keys ì— master01ì˜ id_rsa.pub íŒŒì¼ì„ ë³µì‚¬

- ì£¼ìš” ë¬¸ì œ ë° í•´ê²° ë°©ì•ˆ
    - ë¡œì»¬ì´ë‚˜ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ë¡œì˜ ssh ì ‘ì†ì´ ë˜ì§€ ì•Šì•˜ë˜ ë¬¸ì œ
    â†’ **ë¡œì»¬ì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ì ‘ì† ì‹œ**ì—ëŠ” public IP, **ì¸ìŠ¤í„´ìŠ¤ë¼ë¦¬ì˜ ì ‘ì†**ì—ëŠ” private IPë¥¼ ì„¤ì •í•´ì¤˜ì•¼ ì •ìƒì ìœ¼ë¡œ ì ‘ì†ë¨
    - AWSì˜ ì—°ê²° ì´ìŠˆ â†’ ë³´ì•ˆê·¸ë£¹ì—ì„œ IPv6 port open ::/0
    - Private IPë¡œ SSH ì´ìš© ê°€ëŠ¥ ë° ê¶Œí•œ ì´ìŠˆ â†’ etc/ssh/sshd_config ì— ê¶Œí•œ ì¶”ê°€
    - ì´í›„ ì„¤ì¹˜ ê³¼ì •ì—ì„œ ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ì— ë™ì¼í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•  ë•Œ ë°˜ë³µì ì¸ ì‘ì—…ìœ¼ë¡œ ì¸í•œ íš¨ìœ¨ ì €í•˜ ì˜ˆìƒ â†’ ì¸ìŠ¤í„´ìŠ¤ ì—¬ëŸ¬ ëŒ€ì— ë™ì¼í•œ ëª…ë ¹ì–´ë¥¼ í•œ ë²ˆì— ë³´ë‚¼ ìˆ˜ ìˆëŠ” parallel-ssh ë¥¼ í™œìš©í•´ì„œ ì„¤ì¹˜ ì§„í–‰
    

## 3. Java 8 ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

- Hadoop, Yarn, Spark, ZookeeperëŠ” JVMì—ì„œ ì‘ë™í•˜ê¸° ë•Œë¬¸ì— Java 8 ì„¤ì¹˜ í•„ìš”

- ì„¤ì¹˜ ìˆœì„œ
    1. apt-getì„ ì´ìš©í•´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
    2. Java 8 ì„¤ì¹˜
    3. Java í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

## 4. Hadoop ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

- ì•„í‚¤í…ì³ì—ì„œ ì •í•œ ê° ì¸ìŠ¤í„´ìŠ¤ì˜ ë°ëª¬ë“¤ì„ ì‹¤í–‰ì‹œí‚¤ê¸° ìœ„í•´ Apache Hadoop ì„¤ì¹˜
    - hdfs (Namenode, Datanode, Journalnode, DFSZKfailoverController)
    - yarn (Resourcemanager, Nodemanager)

![Untitled (4)](https://user-images.githubusercontent.com/102719063/203524928-17c40d6a-b895-477d-9ed4-a7c6a8edc718.png)

- Apache Hadoop 3.3.2ë¥¼ ì„¤ì¹˜í•˜ê³  í™˜ê²½ì„¤ì • ì§„í–‰
- ì„¤ì¹˜ ìˆœì„œ
    1. Apache Hadoop 3.3.2 ì„¤ì¹˜ ë° ì••ì¶• í•´ì œ (wget ì‚¬ìš©)
    2. Hadoop í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    3. ì£¼ìš” ì„¤ì • íŒŒì¼ í¸ì§‘ (Apache Hadoop ê³µì‹ ë¬¸ì„œ ì°¸ê³  : [https://hadoop.apache.org/docs/r3.3.4/](https://hadoop.apache.org/docs/r3.3.4/))
        - hdfs-site.xml
        - core-site.xml
        - yarn-site.xml
        - mapred-site.xml
        - hadoop-env.sh
        - workers & masters (namenodeì™€ datanode êµ¬ë¶„)
        

## 5. Spark ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì • - issue

- HDFS ì— ì €ì¥ë˜ì–´ ìˆëŠ” ë°ì´í„°ë¥¼ ë¶„ì‚° ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” spark ì„¤ì¹˜

![Untitled (3)](https://user-images.githubusercontent.com/102719063/203524927-96c56a56-f91f-47d5-8a66-3d41e967bb0f.png)

- ì´ì–´ë“œë¦¼ ê³¼ì • ë™ì•ˆ í•™ìŠµí•œ pythonì„ í™œìš©í•˜ê¸° ìœ„í•´ pyspark ì—°ë™

- Apache Spark 3.2.1ì„ ì„¤ì¹˜í•˜ê³  í™˜ê²½ì„¤ì •ì„ ì§„í–‰
- ì„¤ì¹˜ ìˆœì„œ
    1. Apache Spark 3.2.1 ì„¤ì¹˜ ë° ì••ì¶• í•´ì œ
    2. Python3 & Pyspark ì„¤ì¹˜ ë° íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
    3. Python í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    4. Spark í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    5. ì£¼ìš” ì„¤ì • íŒŒì¼ í¸ì§‘
        - spark-env.sh
        - spark-defaults.conf
        - yarn on spark ë¥¼ ìœ„í•´ cluster masterë¥¼ yarnìœ¼ë¡œ ì„¤ì •
    6. Spark ì‹¤í–‰ ì´ë ¥ ê´€ë¦¬ë¥¼ ìœ„í•œ logs ë””ë ‰í† ë¦¬ ìƒì„±
    7. workers íŒŒì¼ í¸ì§‘ (master03 , slave01, slave02)

- ì£¼ìš” ë¬¸ì œ ë° í•´ê²° ë°©ì•ˆ
    - sparkì™€ pysparkì˜ ë²„ì „ì´ ì¼ì¹˜í•´ì•¼ ì •ìƒì ì¸ ì‹¤í–‰ ê°€ëŠ¥

## 6. Zookeeper ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

- ë¶„ì‚° ì‹œìŠ¤í…œ ë‚´ì—ì„œ ì¤‘ìš”í•œ ìƒíƒœì •ë³´ë‚˜ ì„¤ì •ì •ë³´ë“¤ì„ ìœ ì§€í•˜ê³  ì„œë²„ë“¤ì˜ ìƒíƒœë¥¼ ì²´í¬í•˜ëŠ” ì—­í• 

![Untitled (2)](https://user-images.githubusercontent.com/102719063/203524923-dfcb687f-8617-4104-858a-add1eb7ef507.png)

- Apache Zookeeper 3.8.0ë¥¼ ì„¤ì¹˜í•˜ê³  í™˜ê²½ì„¤ì •ì„ ì§„í–‰
- ì„¤ì¹˜ ìˆœì„œ
    1. Apache Zookeeper 3.8.0 ì„¤ì¹˜ ë° ì••ì¶• í•´ì œ
    2. Zookeeper í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    3. zoo.cfg íŒŒì¼ í¸ì§‘
    4. myid ì„¤ì •
    - ì´ë¦„ì´ â€˜myidâ€™ì¸ íŒŒì¼ì„ ì‘ì„±í•˜ì—¬ Zookeeper ë…¸ë“œë¥¼ ì‹ë³„

## 7. Hive ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

- HDFS ì— ì €ì¥ë˜ì–´ ìˆëŠ” ë°ì´í„°ë¥¼ SQLê³¼ ìœ ì‚¬í•œ ì¿¼ë¦¬ë¡œ ë‹¤ë£° ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ê¸°ìˆ 

![Untitled (7)](https://user-images.githubusercontent.com/102719063/203526005-ae674e71-c744-4ce2-a4fe-ca835e7beb24.png)

- Apache Hive 3.1.3 ë¥¼ ì„¤ì¹˜í•˜ê³  í™˜ê²½ì„¤ì •ì„ ì§„í–‰
- ì„¤ì¹˜ ìˆœì„œ
    1. Apache Hive 3.1.3 ì„¤ì¹˜ ë° ì••ì¶• í•´ì œ
    2. Apache tez 0.10.1 ì„¤ì¹˜ ë° ì••ì¶• í•´ì œ
    3. Hive í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    4. tesz í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    5. Hive MySQL Connector ì„¤ì¹˜
        - Hiveê°€ ì‚¬ìš©í•  mysql ë“œë¼ì´ë²„
    6. Hive ì„¤ì •íŒŒì¼ í¸ì§‘
        - hive-env.sh
        - hive-site.xml
        - hive ì—”ì§„ì„ tezë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì„¤ì • ì¶”ê°€
        - beeline-hs2-connection.xml 
        - í•˜ì´ë¸Œ ì›ê²© ì ‘ì†ì„ ìœ„í•œ beeline ì„œë²„ ì„¤ì •
    7. ë¡œê·¸ íŒŒì¼ ì„¤ì •
    8. ë¡œê·¸ ë””ë ‰í† ë¦¬ ìˆ˜ì •

- ì£¼ìš” ë¬¸ì œ ë° í•´ê²° ë°©ì•ˆ
    - hive ì ‘ì† ì‹œ ì§€ì†ì ìœ¼ë¡œ ì ‘ì† í•´ì œê°€ ë˜ëŠ” ë¬¸ì œ ë°œìƒ â†’ hive session ì‹¤í–‰ í›„ ì—”ì§„ì„ tezë¡œ ì§ì ‘ ì„¤ì •í•´ì¤˜ì•¼ ì•ˆì •ì ì¸ ì‹¤í–‰ ê°€ëŠ¥

# í•˜ë‘¡ í´ëŸ¬ìŠ¤í„° ì‹¤í–‰ ê°€ì´ë“œ

<aside>
ğŸ’¡ **ì•„ë˜ ìˆœì„œëŠ” ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ ì„¤ì¹˜ í›„ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë˜ëŠ”ì§€ í•˜ë‚˜ì”© í™•ì¸í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.
í˜„ì¬ êµ¬ì¶•ëœ í´ëŸ¬ìŠ¤í„°ëŠ” ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì¬ì‹¤í–‰í–ˆì„ ê²½ìš° master01:~/sh/cluster ì—ì„œ format.shì™€ cluster-start-all.shë¥¼ ì°¨ë¡€ë¡œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤.**

</aside>

# 1. Node data ì‚­ì œ

- namenodeì˜ formatê³¼ node ì „ì²´ì˜ ì‹¤í–‰ì„ ìœ„í•´ ì¸ìŠ¤í„´ìŠ¤ ì¤‘ì§€ ì „ ë‚¨ì•„ìˆë˜ ì„ì‹œ ë°ì´í„° ì‚­ì œ í•„ìš”
- master01/02/03 : journalnode data ì‚­ì œ
- master03/slave01/slave02 : datanode data ì‚­ì œ

```bash
# master01/02/03 journalnode data ì‚­ì œ
$ parallel-ssh -h ~/.ssh/master -i "cd /usr/local/hadoop/data/dfs/journalnode && rm -rf team06-hadoop-cluster"

# slave01/02 datanode data ì‚­ì œ
$ parallel-ssh -h ~/.ssh/slave -i "cd /usr/local/hadoop/data/data && rm -rf current"

# master03 datanode data ì‚­ì œ
$ ssh master03
$ cd /usr/local/hadoop/data/data && rm -rf current
$ exit
```

# 2. Namenode format & test starting - issue

- namenode í¬ë§· (journalnodeê°€ ì‹¤í–‰ëœ ìƒíƒœì—ì„œ ê°€ëŠ¥) í›„ master01/02ì—ì„œ namenode ì‹¤í–‰ í™•ì¸

```bash
# journalnode ì‹¤í–‰
$ parallel-ssh -h ~/.ssh/master "hdfs --daemon start journalnode"

# namenode format (master01 ì—ì„œë§Œ ì§„í–‰)
$ hdfs namenode -format

# namenode ì‹¤í–‰
$ hdfs --daemon start namenode

# master02ì—ì„œ hdfs standby namenode ì‹¤í–‰
$ ssh master02
$ hdfs namenode -bootstrapStandby

# master02 namenode ì‹¤í–‰
$ hdfs --daemon start namenode
$ exit

# master01 ì—ì„œ jpsë¡œ master01/02ì˜ namenode ì‹¤í–‰ í™•ì¸
$ sh ~/sh/jps-check-all.sh
[1] 01:53:18 [SUCCESS] master02
1380 JournalNode
1756 Jps
1596 NameNode
[2] 01:53:18 [SUCCESS] master03
2103 JournalNode
2237 Jps
[3] 01:53:18 [SUCCESS] slave01
1387 Jps
[4] 01:53:18 [SUCCESS] slave02
1211 Jps
[5] 01:53:18 [SUCCESS] master01
1363 JournalNode
1509 NameNode
1722 Jps

# Zookeeper ì‹¤í–‰ í›„ start-dfs.shë¡œ í•œë²ˆì— ì „ì²´ node ì‹¤í–‰ì„ ìœ„í•´ ì‹¤í–‰ ì¤‘ì§€
$ cd /usr/local/hadoop/sbin && stop-dfs.sh
```

- ì£¼ìš” ë¬¸ì œ ë° í•´ê²° ë°©ì•ˆ
    - dfs.namenode.name.dir ì•„ë˜ VERSION íŒŒì¼ì—ì„œ í´ëŸ¬ìŠ¤í„° IDê°€ ì¼ì¹˜í•˜ì§€ ì•Šì•„ namenodeê°€ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ
    â†’ journalnodeì™€ datanode ì˜ VERSION íŒŒì¼ì„ ì‚­ì œí•œ í›„ namenode formatì„ ì§„í–‰í•˜ë©´ í´ëŸ¬ìŠ¤í„° IDê°€ ì‹ ê·œë¡œ ìƒì„±ë˜ì–´ ì •ìƒì ì¸ ì‹¤í–‰ ê°€ëŠ¥

# 3. Zookeeper í´ëŸ¬ìŠ¤í„° ì‹¤í–‰

- parallel-ssh ë¡œ zookeeper ì‹¤í–‰ í›„ status í™•ì¸

```bash
# master01/02/03
$ parallel-ssh -h ~/.ssh/master -i "/usr/local/zookeeper/bin/zkServer.sh start"
[1] 01:58:23 [SUCCESS] master01
Starting zookeeper ... STARTED
Stderr: ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
[2] 01:58:23 [SUCCESS] master02
Starting zookeeper ... STARTED
Stderr: ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
[3] 01:58:24 [SUCCESS] master03
Starting zookeeper ... STARTED
Stderr: ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg

# Zookeeper status í™•ì¸
$ parallel-ssh -h ~/.ssh/master -i "/usr/local/zookeeper/bin/zkServer.sh status"
[1] 01:58:35 [SUCCESS] master01
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
Stderr: ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
[2] 01:58:35 [SUCCESS] master03
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: leader
Stderr: ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
[3] 01:58:35 [SUCCESS] master02
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
Stderr: ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg

ubuntu@team06-master01:~$ sh ~/sh/jps-check-all.sh
[1] 02:30:22 [SUCCESS] master02
3682 Jps
2287 QuorumPeerMain
[2] 02:30:22 [SUCCESS] slave01
2235 Jps
[3] 02:30:22 [SUCCESS] slave02
2068 Jps
[4] 02:30:22 [SUCCESS] master03
2654 QuorumPeerMain
3743 Jps
[5] 02:30:22 [SUCCESS] master01
2505 QuorumPeerMain
4495 Jps
```

- zookeeper ì‹¤í–‰ í›„ zkfc format ì§„í–‰

```bash
hdfs zkfc -format
```

# 4. Hadoop í´ëŸ¬ìŠ¤í„° ì‹¤í–‰

- namenode(master01/02)
datanodes(master01, slave01/02)
journalnode(master01/02/03)
ZK Failvoer Controller(master01/02)
start-dfs.sh ë¡œ í•œë²ˆì— ì‹¤í–‰

```bash
# master01 ì—ì„œ ì‹¤í–‰ í›„ jps í™•ì¸
ubuntu@team06-master01:~$ cd /usr/local/hadoop/sbin && start-dfs.sh
Starting namenodes on [master01 master02]
Starting datanodes
Starting journal nodes [master02 master03 master01]
Starting ZK Failover Controllers on NN hosts [master01 master02]

# jps check
ubuntu@team06-master01:~$ sh ~/sh/jps-check-all.sh
[1] 02:03:52 [SUCCESS] slave01
1968 Jps
1814 DataNode
[2] 02:03:52 [SUCCESS] master02
2786 JournalNode
2965 DFSZKFailoverController
3110 Jps
2647 NameNode
2287 QuorumPeerMain
[3] 02:03:52 [SUCCESS] master03
3177 JournalNode
3020 DataNode
3342 Jps
2654 QuorumPeerMain
[4] 02:03:52 [SUCCESS] master01
3447 DFSZKFailoverController
2952 NameNode
2505 QuorumPeerMain
3211 JournalNode
3581 Jps
[5] 02:03:52 [SUCCESS] slave02
1801 Jps
1658 DataNode
```

# 5. Yarn í´ëŸ¬ìŠ¤í„° ì‹¤í–‰ - issue

- Starting resourcemanagers on [ master01 master02]
Starting nodemanagers

```bash
ubuntu@team06-master01:~$ cd /usr/local/hadoop/sbin && start-yarn.sh
Starting resourcemanagers on [ master01 master02]
Starting nodemanagers

# jps check
ubuntu@team06-master01:~$ sh ~/sh/jps-check-all.sh
[1] 02:37:05 [SUCCESS] master02
4688 Jps
3936 JournalNode
4389 ResourceManager
3797 NameNode
4117 DFSZKFailoverController
2287 QuorumPeerMain
[2] 02:37:05 [SUCCESS] slave01
2625 NodeManager
2359 DataNode
2795 Jps
[3] 02:37:05 [SUCCESS] slave02
2456 NodeManager
2653 Jps
2191 DataNode
[4] 02:37:05 [SUCCESS] master03
4465 Jps
4289 NodeManager
4020 JournalNode
3862 DataNode
2654 QuorumPeerMain
[5] 02:37:05 [SUCCESS] master01
4680 NameNode
5624 ResourceManager
2505 QuorumPeerMain
5177 DFSZKFailoverController
4939 JournalNode
5822 Jps
```

- ì£¼ìš” ë¬¸ì œ ë° í•´ê²° ë°©ì•ˆ
    - resourcemanagerê°€ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ” ë¬¸ì œ
    â†’ ë¡œê·¸ íŒŒì¼ì„ í™•ì¸í•´ì„œ resourcemanager ì‹¤í–‰ì— í•„ìš”í•œ java í´ë˜ìŠ¤ê°€ í¬í•¨ë˜ì–´ ìˆëŠ” jar íŒŒì¼ì´ ëˆ„ë½ë˜ì–´ ìƒê¸´ ë¬¸ì œë¼ëŠ” ê²ƒì„ íŒŒì•…
    â†’ /usr/local/hadoop/share/hadoop/yarn - hadoop ì„¤ì¹˜ í›„ ì••ì¶• í•´ì œê°€ ì œëŒ€ë¡œ ë˜ì§€ ì•Šì•„ í•´ë‹¹ ê²½ë¡œì— íŒŒì¼ ëˆ„ë½
    â†’ ì •ìƒì ìœ¼ë¡œ ì••ì¶• í•´ì œëœ íŒŒì¼ì„ í•´ë‹¹ ê²½ë¡œì— ì¶”ê°€í•´ì„œ í•´ê²°

# 6. Hive ì‹¤í–‰

- utilnode(master03)ì—ì„œ ì‹¤í–‰ í™•ì¸

```bash
ubuntu@team06-master03:~$ ssh master03
ubuntu@team06-master03:~$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.3.2/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 287e0e49-08dd-4430-a0f2-5a0ac6b85184

Logging initialized using configuration in file:/usr/local/apache-hive-3.1.2-bin/conf/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive>
```

- hiveserver2 background ì‹¤í–‰
- ë¡œê·¸ê°€ ë‚¨ì§€ ì•Šë„ë¡ nohupê°€ ìˆ˜í–‰í•œ ëª…ë ¹ì„ /dev/nullë¡œ ë³´ëƒ„

```bash
ubuntu@team06-master03:~$ nohup $HIVE_HOME/bin/hiveserver2 & > /dev/null
[1] 4716
ubuntu@team06-master03:~$ nohup: ignoring input and appending output to 'nohup.out'
jobs
[1]+  Running                 nohup $HIVE_HOME/bin/hiveserver2 &
```

+background í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ : grep ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•´ì„œ pidë¥¼ í™•ì¸í•œ í›„ kill

```bash
ubuntu@team06-master03:~$ ps -ef | grep hiveserver
ubuntu      4866    4573 30 02:59 pts/0    00:00:19 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dproc_jar -Djava.library.path=/usr/local/hadoop/lib/native -Djava.library.path=/usr/local/hadoop/lib/native -Dproc_hiveserver2 -Dlog4j.configurationFile=hive-log4j2.properties -Djava.util.logging.config.file=/usr/local/hive/conf/parquet-logging.properties -Djline.terminal=jline.UnsupportedTerminal -Dyarn.log.dir=/usr/local/hadoop/logs -Dyarn.log.file=hadoop.log -Dyarn.home.dir=/usr/local/hadoop -Dyarn.root.logger=INFO,console -Xmx256m -Dhadoop.log.dir=/usr/local/hadoop/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/local/hadoop -Dhadoop.id.str=ubuntu -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /usr/local/hive/lib/hive-service-3.1.2.jar org.apache.hive.service.server.HiveServer2
ubuntu      4986    4573  0 03:00 pts/0    00:00:00 grep --color=auto hiveserver
ubuntu@team06-master03:~$ kill -9 4866
ubuntu@team06-master03:~$ jobs
[1]+  Killed                  nohup $HIVE_HOME/bin/hiveserver2
```

- beeline ì ‘ì† - 10.13 12:07 ì˜¤ë¥˜ë¡œ ì ‘ì†ë˜ì§€ ì•ŠìŒ.

```bash
ubuntu@team06-master03:~$ $HIVE_HOME/bin/beeline -u jdbc:hive2://localhost:10000
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.3.2/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://localhost:10000
22/10/13 03:08:22 [main]: WARN jdbc.HiveConnection: Failed to connect to localhost:10000
Could not open connection to the HS2 server. Please check the server URI and if the URI is correct, then ask the administrator to check the server status.
Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0)
Beeline version 3.1.2 by Apache Hive
```

â†’ ëª¨ë“  í´ëŸ¬ìŠ¤í„°ì™€ hiveserver2ê¹Œì§€ ì‹¤í–‰í•œ í›„ hiveserver2ê°€ ì˜¬ë¼ì˜¤ê¸°ê¹Œì§€ ëŒ€ê¸° í•œ í›„ ì ‘ì† ê°€ëŠ¥ (config íŒŒì¼ì— ì˜¤ë¥˜ëŠ” ë”±íˆ ì—†ì—ˆë˜ ê²ƒìœ¼ë¡œ íŒë‹¨ë¨)

```bash
ubuntu@team06-master03:/usr/local/hive/conf$ $HIVE_HOME/bin/beeline -u jdbc:hive2://localhost:10000
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.3.2/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 3.1.2)
Driver: Hive JDBC (version 3.1.2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.2 by Apache Hive
0: jdbc:hive2://localhost:10000>
```

# 7. Spark í´ëŸ¬ìŠ¤í„° ì‹¤í–‰

- master03ì—ì„œ ì‹¤í–‰ (workers : master03, slave01, slave02)

```bash
# EC2 Ubuntu terminal(master03)
ubuntu@team06-master03:~$ $SPARK_HOME/sbin/start-all.sh
starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.master.Master-1-team06-master03.hadoop.com.out
master03: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-team06-master03.hadoop.com.out
slave01: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-team06-slave01.hadoop.com.out
slave02: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-team06-slave02.hadoop.com.out
```

- pyspark ì‹¤í–‰

```bash
ubuntu@team06-master03:~$ /usr/local/spark/bin/pyspark
Python 3.8.10 (default, Jun 22 2022, 20:18:18)
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
Warning: Ignoring non-Spark config property: "
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
2022-10-13 03:11:20,852 WARN conf.HiveConf: HiveConf of name hive.metastore.schema.verfication does not exist
2022-10-13 03:11:20,971 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-10-13 03:11:23,560 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.1
      /_/

Using Python version 3.8.10 (default, Jun 22 2022 20:18:18)
Spark context Web UI available at http://master03:4040
Spark context available as 'sc' (master = yarn, app id = application_1665628591243_0001).
SparkSession available as 'spark'.
>>>
```

- jupyter notebook (pyspark ì‹¤í–‰ ì‹œ ìë™ ì‹¤í–‰)

```jsx
ubuntu@team06-master03:~/pyspark$ pyspark
[I 16:53:11.774 NotebookApp] Serving notebooks from local directory: /home/ubuntu/pyspark
[I 16:53:11.774 NotebookApp] Jupyter Notebook 6.5.1 is running at:
[I 16:53:11.774 NotebookApp] http://172.31.40.7:8911/
[I 16:53:11.774 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
```

â†’ {master03 public ip}://8911ë¡œ jupyter notebook ì ‘ì†
password : team06

# Appendix

- master01 ì„œë²„ì˜ ~/sh/cluster/jps-check-all.sh : ëª¨ë“  nodeì˜ jps í™•ì¸

```bash
ubuntu@team06-master01:~/sh/cluster$ cat jps-check-all.sh
# Jps chaek
parallel-ssh -h ~/.ssh/all -i "sudo jps"
```

- ëª¨ë“  í´ëŸ¬ìŠ¤í„° ì‹¤í–‰ í›„ ìµœì¢… jps

```bash
ubuntu@team06-master01:~$ sh ~/sh/cluster/jps-check-all.sh
[1] 03:13:16 [SUCCESS] slave02
2882 Worker
2456 NodeManager
3199 Jps
2191 DataNode
[2] 03:13:16 [SUCCESS] slave01
2625 NodeManager
2359 DataNode
3288 Jps
2972 Worker
[3] 03:13:16 [SUCCESS] master03
4289 NodeManager
6483 Jps
4020 JournalNode
3862 DataNode
6170 Worker
6011 Master
4990 RunJar
2654 QuorumPeerMain
[4] 03:13:16 [SUCCESS] master01
6261 Jps
4680 NameNode
5624 ResourceManager
2505 QuorumPeerMain
5177 DFSZKFailoverController
4939 JournalNode
[5] 03:13:16 [SUCCESS] master02
3936 JournalNode
4389 ResourceManager
3797 NameNode
4117 DFSZKFailoverController
5382 Jps
2287 QuorumPeerMain
```

- sparkì™€ hiveë¥¼ ì œì™¸í•œ ëª¨ë“  í´ëŸ¬ìŠ¤í„° ì‹¤í–‰(ì¤‘ì§€) ì‰˜ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
ê²½ë¡œ : master01:~/sh/cluster
    - cluster-start-all.sh
    
    ```bash
    # Zookeeper
    echo " ---------------- Zookeeper -----------------"
    parallel-ssh -h ~/.ssh/master -i "/usr/local/zookeeper/bin/zkServer.sh start"
    
    # Hadoop
    echo " ------------------ Hadoop ------------------"
    start-dfs.sh
    
    # Yarn
    echo " ------------------- yarn -------------------"
    start-yarn.sh
    
    # Jps check
    echo " -------------------- jps -------------------"
    sh ~/sh/cluster/jps-check-all.sh
    ```
    
    - cluster-stop-all.sh
    
    ```bash
    # Zookeeper
    echo " ---------------- Zookeeper -----------------"
    parallel-ssh -h ~/.ssh/master -i "/usr/local/zookeeper/bin/zkServer.sh stop"
    
    # Hadoop
    echo " ------------------ Hadoop ------------------"
    stop-dfs.sh
    
    # Yarn
    echo " ------------------- yarn -------------------"
    #echo "Stopping nodemanager"
    #parallel-ssh -h ~/.ssh/worker "yarn --daemon stop nodemanager"
    #echo "Stopping resourcemanager [master01 master02]"
    #parallel-ssh -h ~/.ssh/master "yarn --daemon stop resourcemanager"
    yarn-stop.sh
    
    # Jps check
    echo " -------------------- jps -------------------"
    sh ~/sh/cluster/jps-check-all.sh
    ```
    

# Pysparkë¥¼ í™œìš©í•œ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì • - issue

- raw dataë¥¼  ëª¨ë¸ì´ í•™ìŠµ ê°€ëŠ¥í•œ ìƒíƒœì˜ ë°ì´í„°ë¡œ ì²˜ë¦¬
- ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ HDFSì— ì €ì¥í•œ í›„ feture storeì¸ Amazon S3ì— dvcë¥¼ ì´ìš©í•œ ì „ì†¡ ê¸°ëŠ¥ êµ¬í˜„

- ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ì— ì§‘ì¤‘í•˜ê¸° ìœ„í•´ ê°„ë‹¨í•œ EDA í›„ ì „ì²˜ë¦¬ ì§„í–‰

![Untitled (1)](https://user-images.githubusercontent.com/102719063/203524914-c56c948b-28c7-46da-bcd4-97fee43cd8a1.png)

- EDA ê²°ê³¼ì— ë”°ë¥¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ ì½”ë“œ ì‘ì„±
    - def column_selection
    - ì‚¬ìš©í•  ì»¬ëŸ¼ ì¶”ì¶œ
    - def rm_missingValue
    - ê²°ì¸¡ì¹˜ ì œê±°
    - def rm_outlier
    - ì´ìƒì¹˜ ì œê±°
    - def datetime_convert
    - datetime ì„ ìš”ì¼, ì‹œê°„, ë¶„ìœ¼ë¡œ ë³€í™˜í•´ì„œ ìƒˆë¡œìš´ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
    - def datetime_diff
    - pikup datetimeê³¼ dropoff datetimeì˜ ì°¨ì´ë¥¼ êµ¬í•´ì„œ íƒì‹œë¥¼ ì´ìš©í•œ ì‹œê°„ì„ ë‚˜íƒ€ë‚´ëŠ” ìƒˆë¡œìš´ ì»¬ëŸ¼ìœ¼ë¡œ ì¶”ê°€
    - def drop_columns
    - í•™ìŠµì— ìš©ì´í•˜ì§€ ì•Šì€ datetime ì»¬ëŸ¼ ì‚­ì œ

- ì „ì²˜ë¦¬ëœ spark ë°ì´í„°í”„ë ˆì„ì„ hdfsì— parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥

![Untitled](https://user-images.githubusercontent.com/102719063/203524936-bcea7dba-901d-49db-b058-2674dd139ecf.png)

- ì£¼ìš” ë¬¸ì œ ë° í•´ê²° ë°©ì•ˆ
    - 2017ë…„ë„ì˜ ì „ì›” íŒŒì¼ì„ í•™ìŠµì‹œí‚¨ ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ dataframeì„ í•©ì¹œ í›„ hdfsì— íŒŒì¼ë¡œ ì €ì¥í•˜ë ¤ í–ˆìœ¼ë‚˜ ë¶„ë¦¬ëœ parquet íŒŒì¼ë¡œ ì €ì¥ë¨.
    â†’ parquet íŒŒì¼ì„ í•˜ë‚˜ë¡œ repartition í•˜ëŠ” ì˜µì…˜ì„ ì¶”ê°€í•œ ì½”ë“œë¥¼ í†µí•´ í•˜ë‚˜ì˜ parquet íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ
