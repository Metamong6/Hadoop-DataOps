# 데이터 파이프라인 구축 과정

# 목표

> 원본 데이터를 저장하고 전처리해서 모델이 학습하기 직전의 데이터 상태까지 만든 후 feature store에 저장하는 파이프라인을 Hadoop 클러스터로 구축
> 
- 파이프라인 플로우
데이터 소스 → 데이터 웨어하우스 → 데이터 분석/처리 → feature store
- 파이프라인 요소
    1. 데이터 소스 : NYC taxi data 
    2. 데이터 웨어하우스 : HDFS (Hadoop)
    3. 데이터 처리 : yarn on spark (pyspark)
    4. feature store : AMAZON S3


# 클러스터 아키텍쳐

- 하둡 클러스터 아키텍쳐
    
    ![Untitled (6)](https://user-images.githubusercontent.com/102719063/203525067-bbf5ebdb-a630-4c10-96ae-5fa5d4249e68.png)
    
    - MasterNode # 1,2
        - **Namenode** 
          - hdfs에 있는 데이터를 datanode에 분산시키고 관리하는 기능을 담당
          - slave에 해당하는 datanode에게 I/O를 담당하고 datanode의 이상유무를 확인
        - **Journalnode**
          - Namnode의 edits 정보를 저장하고 공유하는 기능을 수행
        - **Failover Controller**
          - active Namenode에 문제가 발생하는 경우 해당 노드를 내리고 standby 상태에 있는 다른 Namenode를 active로 전환
        - **ResourceManager (Yarn)**
          - 어느 작업에 자원을 할당할지 결정해서 cluster의 활용을 최적화
          - 작업에 대한 처리 요청을 받으면, Request들의 일부를 해당하는 NodeManager로 전달
        - **Zookeeper**
          - zookeeper 설치 내용에서 역할 설명
        
    - Utility Node # 1
        - **Datanode**
          - 데이터노드는 클라이언트나 네임노드의 요청이 있을 때 블록을 저장하고 탐색하며, 저장하고 있는 블록의 목록을 주기적으로 네임노드에 보고
        - **NodeManager (Yarn)**
          - 클러스터에 속한 노드들에서 컨테이너를 실행하고, 각 컨테이너의 리소스 사용량을 모니터링하고 그것의 상태를 ResourceManager에 보고하는 역할을 담당
        - **Zookeeper**
        - **Hiveserver2, Hive metastore**
          - Hive 설치 내용에서 역할 설명
        
    - Data Node # 1,2
        - **Datanode**
        - **Nodemanager (Yarn)**

# 하둡 클러스터 구축 과정

## 1. AWS EC2 인스턴스 배포

- 하둡 클러스터 구성을 위해 EC2 인스턴스 5대 배포
- 실질적으로 data가 저장되는 **data node**는 스토리지 용량을 50GB로 높게 설정
- spark와 hive를 실행시키는 **util node**는 원활한 작업 처리를 위해 vCPU 수를 4개로 설정 (인스턴스 유형 - t3a.xlarge)

|  | Master node # 1,2 | Util node # 1 | Data node # 1,2 |
| --- | --- | --- | --- |
| 인스턴스 유형 | t3a.medium | t3a.xlarge | t3a.medium |
| 스토리지 | 30GB | 50GB | 50GB |
| vCPU 수 | 2 | 4 | 2 |
| OS | Ubuntu 20.04 | Ubuntu 20.04 | Ubuntu 20.04 |

## 2. SSH 및 호스트 이름 설정 - issue

- 5개의 인스턴스에서 하둡이 동작하려면 인스턴스끼리 데이터를 주고 받을 수 있도록 연결해야함.

- 설정 순서
    1. 작업을 진행하는 PC(Mac)에서 SSH 설정 후 인스턴스 배포 시 다운로드 받은 pem 파일을 이용하여 배포한 인스턴스에 접속
    2. 모든 인스턴스의 /etc/hosts 파일에 아래 이미지처럼 각 인스턴스의 호스트 이름과 private IP 추가 
    - 인스턴스끼리 SSH 통신이 가능하도록 설정하고 각 서버의 호스트 이름을 설정
        
        ![Untitled (5)](https://user-images.githubusercontent.com/102719063/203524933-4e973636-a8e2-4267-b171-bab594a55ec6.png)
        
    
    3. master01에서 ssh-keygen으로 생성된 키를 모든 인스턴스에 배포
    - ~/.ssh/authorized_keys 에 master01의 id_rsa.pub 파일을 복사

- 주요 문제 및 해결 방안
    - 로컬이나 인스턴스에서 인스턴스로의 ssh 접속이 되지 않았던 문제
    → **로컬에서 인스턴스 접속 시**에는 public IP, **인스턴스끼리의 접속**에는 private IP를 설정해줘야 정상적으로 접속됨
    - AWS의 연결 이슈 → 보안그룹에서 IPv6 port open ::/0
    - Private IP로 SSH 이용 가능 및 권한 이슈 → etc/ssh/sshd_config 에 권한 추가
    - 이후 설치 과정에서 여러 인스턴스에 동일한 패키지를 설치할 때 반복적인 작업으로 인한 효율 저하 예상 → 인스턴스 여러 대에 동일한 명령어를 한 번에 보낼 수 있는 parallel-ssh 를 활용해서 설치 진행
    

## 3. Java 8 설치 및 환경 설정

- Hadoop, Yarn, Spark, Zookeeper는 JVM에서 작동하기 때문에 Java 8 설치 필요

- 설치 순서
    1. apt-get을 이용해 필요한 라이브러리 설치
    2. Java 8 설치
    3. Java 환경 변수 설정

## 4. Hadoop 설치 및 환경 설정

- 아키텍쳐에서 정한 각 인스턴스의 데몬들을 실행시키기 위해 Apache Hadoop 설치
    - hdfs (Namenode, Datanode, Journalnode, DFSZKfailoverController)
    - yarn (Resourcemanager, Nodemanager)

![Untitled (4)](https://user-images.githubusercontent.com/102719063/203524928-17c40d6a-b895-477d-9ed4-a7c6a8edc718.png)

- Apache Hadoop 3.3.2를 설치하고 환경설정 진행
- 설치 순서
    1. Apache Hadoop 3.3.2 설치 및 압축 해제 (wget 사용)
    2. Hadoop 환경 변수 설정
    3. 주요 설정 파일 편집 (Apache Hadoop 공식 문서 참고 : [https://hadoop.apache.org/docs/r3.3.4/](https://hadoop.apache.org/docs/r3.3.4/))
        - hdfs-site.xml
        - core-site.xml
        - yarn-site.xml
        - mapred-site.xml
        - hadoop-env.sh
        - workers & masters (namenode와 datanode 구분)
        

## 5. Spark 설치 및 환경 설정 - issue

- HDFS 에 저장되어 있는 데이터를 분산 처리할 수 있는 spark 설치

![Untitled (3)](https://user-images.githubusercontent.com/102719063/203524927-96c56a56-f91f-47d5-8a66-3d41e967bb0f.png)

- 이어드림 과정 동안 학습한 python을 활용하기 위해 pyspark 연동

- Apache Spark 3.2.1을 설치하고 환경설정을 진행
- 설치 순서
    1. Apache Spark 3.2.1 설치 및 압축 해제
    2. Python3 & Pyspark 설치 및 파이썬 라이브러리 설치
    3. Python 환경 변수 설정
    4. Spark 환경 변수 설정
    5. 주요 설정 파일 편집
        - spark-env.sh
        - spark-defaults.conf
        - yarn on spark 를 위해 cluster master를 yarn으로 설정
    6. Spark 실행 이력 관리를 위한 logs 디렉토리 생성
    7. workers 파일 편집 (master03 , slave01, slave02)

- 주요 문제 및 해결 방안
    - spark와 pyspark의 버전이 일치해야 정상적인 실행 가능

## 6. Zookeeper 설치 및 환경 설정

- 분산 시스템 내에서 중요한 상태정보나 설정정보들을 유지하고 서버들의 상태를 체크하는 역할

![Untitled (2)](https://user-images.githubusercontent.com/102719063/203524923-dfcb687f-8617-4104-858a-add1eb7ef507.png)

- Apache Zookeeper 3.8.0를 설치하고 환경설정을 진행
- 설치 순서
    1. Apache Zookeeper 3.8.0 설치 및 압축 해제
    2. Zookeeper 환경 변수 설정
    3. zoo.cfg 파일 편집
    4. myid 설정
    - 이름이 ‘myid’인 파일을 작성하여 Zookeeper 노드를 식별

## 7. Hive 설치 및 환경 설정

- HDFS 에 저장되어 있는 데이터를 SQL과 유사한 쿼리로 다룰 수 있게 해주는 기술

![Untitled (7)](https://user-images.githubusercontent.com/102719063/203526005-ae674e71-c744-4ce2-a4fe-ca835e7beb24.png)

- Apache Hive 3.1.3 를 설치하고 환경설정을 진행
- 설치 순서
    1. Apache Hive 3.1.3 설치 및 압축 해제
    2. Apache tez 0.10.1 설치 및 압축 해제
    3. Hive 환경 변수 설정
    4. tesz 환경 변수 설정
    5. Hive MySQL Connector 설치
        - Hive가 사용할 mysql 드라이버
    6. Hive 설정파일 편집
        - hive-env.sh
        - hive-site.xml
        - hive 엔진을 tez로 사용하기 위한 설정 추가
        - beeline-hs2-connection.xml 
        - 하이브 원격 접속을 위한 beeline 서버 설정
    7. 로그 파일 설정
    8. 로그 디렉토리 수정

- 주요 문제 및 해결 방안
    - hive 접속 시 지속적으로 접속 해제가 되는 문제 발생 → hive session 실행 후 엔진을 tez로 직접 설정해줘야 안정적인 실행 가능

# 하둡 클러스터 실행 가이드

<aside>
💡 **아래 순서는 모든 컴포넌트를 설치 후 정상적으로 실행되는지 하나씩 확인하는 작업입니다.
현재 구축된 클러스터는 인스턴스를 재실행했을 경우 master01:~/sh/cluster 에서 format.sh와 cluster-start-all.sh를 차례로 실행하면 됩니다.**

</aside>

# 1. Node data 삭제

- namenode의 format과 node 전체의 실행을 위해 인스턴스 중지 전 남아있던 임시 데이터 삭제 필요
- master01/02/03 : journalnode data 삭제
- master03/slave01/slave02 : datanode data 삭제

```bash
# master01/02/03 journalnode data 삭제
$ parallel-ssh -h ~/.ssh/master -i "cd /usr/local/hadoop/data/dfs/journalnode && rm -rf team06-hadoop-cluster"

# slave01/02 datanode data 삭제
$ parallel-ssh -h ~/.ssh/slave -i "cd /usr/local/hadoop/data/data && rm -rf current"

# master03 datanode data 삭제
$ ssh master03
$ cd /usr/local/hadoop/data/data && rm -rf current
$ exit
```

# 2. Namenode format & test starting - issue

- namenode 포맷 (journalnode가 실행된 상태에서 가능) 후 master01/02에서 namenode 실행 확인

```bash
# journalnode 실행
$ parallel-ssh -h ~/.ssh/master "hdfs --daemon start journalnode"

# namenode format (master01 에서만 진행)
$ hdfs namenode -format

# namenode 실행
$ hdfs --daemon start namenode

# master02에서 hdfs standby namenode 실행
$ ssh master02
$ hdfs namenode -bootstrapStandby

# master02 namenode 실행
$ hdfs --daemon start namenode
$ exit

# master01 에서 jps로 master01/02의 namenode 실행 확인
$ sh ~/sh/jps-check-all.sh
[1] 01:53:18 [SUCCESS] master02
1380 JournalNode
1756 Jps
1596 NameNode
[2] 01:53:18 [SUCCESS] master03
2103 JournalNode
2237 Jps
[3] 01:53:18 [SUCCESS] slave01
1387 Jps
[4] 01:53:18 [SUCCESS] slave02
1211 Jps
[5] 01:53:18 [SUCCESS] master01
1363 JournalNode
1509 NameNode
1722 Jps

# Zookeeper 실행 후 start-dfs.sh로 한번에 전체 node 실행을 위해 실행 중지
$ cd /usr/local/hadoop/sbin && stop-dfs.sh
```

- 주요 문제 및 해결 방안
    - dfs.namenode.name.dir 아래 VERSION 파일에서 클러스터 ID가 일치하지 않아 namenode가 실행되지 않는 문제
    → journalnode와 datanode 의 VERSION 파일을 삭제한 후 namenode format을 진행하면 클러스터 ID가 신규로 생성되어 정상적인 실행 가능

# 3. Zookeeper 클러스터 실행

- parallel-ssh 로 zookeeper 실행 후 status 확인

```bash
# master01/02/03
$ parallel-ssh -h ~/.ssh/master -i "/usr/local/zookeeper/bin/zkServer.sh start"
[1] 01:58:23 [SUCCESS] master01
Starting zookeeper ... STARTED
Stderr: ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
[2] 01:58:23 [SUCCESS] master02
Starting zookeeper ... STARTED
Stderr: ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
[3] 01:58:24 [SUCCESS] master03
Starting zookeeper ... STARTED
Stderr: ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg

# Zookeeper status 확인
$ parallel-ssh -h ~/.ssh/master -i "/usr/local/zookeeper/bin/zkServer.sh status"
[1] 01:58:35 [SUCCESS] master01
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
Stderr: ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
[2] 01:58:35 [SUCCESS] master03
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: leader
Stderr: ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg
[3] 01:58:35 [SUCCESS] master02
Client port found: 2181. Client address: localhost. Client SSL: false.
Mode: follower
Stderr: ZooKeeper JMX enabled by default
Using config: /usr/local/zookeeper/bin/../conf/zoo.cfg

ubuntu@team06-master01:~$ sh ~/sh/jps-check-all.sh
[1] 02:30:22 [SUCCESS] master02
3682 Jps
2287 QuorumPeerMain
[2] 02:30:22 [SUCCESS] slave01
2235 Jps
[3] 02:30:22 [SUCCESS] slave02
2068 Jps
[4] 02:30:22 [SUCCESS] master03
2654 QuorumPeerMain
3743 Jps
[5] 02:30:22 [SUCCESS] master01
2505 QuorumPeerMain
4495 Jps
```

- zookeeper 실행 후 zkfc format 진행

```bash
hdfs zkfc -format
```

# 4. Hadoop 클러스터 실행

- namenode(master01/02)
datanodes(master01, slave01/02)
journalnode(master01/02/03)
ZK Failvoer Controller(master01/02)
start-dfs.sh 로 한번에 실행

```bash
# master01 에서 실행 후 jps 확인
ubuntu@team06-master01:~$ cd /usr/local/hadoop/sbin && start-dfs.sh
Starting namenodes on [master01 master02]
Starting datanodes
Starting journal nodes [master02 master03 master01]
Starting ZK Failover Controllers on NN hosts [master01 master02]

# jps check
ubuntu@team06-master01:~$ sh ~/sh/jps-check-all.sh
[1] 02:03:52 [SUCCESS] slave01
1968 Jps
1814 DataNode
[2] 02:03:52 [SUCCESS] master02
2786 JournalNode
2965 DFSZKFailoverController
3110 Jps
2647 NameNode
2287 QuorumPeerMain
[3] 02:03:52 [SUCCESS] master03
3177 JournalNode
3020 DataNode
3342 Jps
2654 QuorumPeerMain
[4] 02:03:52 [SUCCESS] master01
3447 DFSZKFailoverController
2952 NameNode
2505 QuorumPeerMain
3211 JournalNode
3581 Jps
[5] 02:03:52 [SUCCESS] slave02
1801 Jps
1658 DataNode
```

# 5. Yarn 클러스터 실행 - issue

- Starting resourcemanagers on [ master01 master02]
Starting nodemanagers

```bash
ubuntu@team06-master01:~$ cd /usr/local/hadoop/sbin && start-yarn.sh
Starting resourcemanagers on [ master01 master02]
Starting nodemanagers

# jps check
ubuntu@team06-master01:~$ sh ~/sh/jps-check-all.sh
[1] 02:37:05 [SUCCESS] master02
4688 Jps
3936 JournalNode
4389 ResourceManager
3797 NameNode
4117 DFSZKFailoverController
2287 QuorumPeerMain
[2] 02:37:05 [SUCCESS] slave01
2625 NodeManager
2359 DataNode
2795 Jps
[3] 02:37:05 [SUCCESS] slave02
2456 NodeManager
2653 Jps
2191 DataNode
[4] 02:37:05 [SUCCESS] master03
4465 Jps
4289 NodeManager
4020 JournalNode
3862 DataNode
2654 QuorumPeerMain
[5] 02:37:05 [SUCCESS] master01
4680 NameNode
5624 ResourceManager
2505 QuorumPeerMain
5177 DFSZKFailoverController
4939 JournalNode
5822 Jps
```

- 주요 문제 및 해결 방안
    - resourcemanager가 실행되지 않는 문제
    → 로그 파일을 확인해서 resourcemanager 실행에 필요한 java 클래스가 포함되어 있는 jar 파일이 누락되어 생긴 문제라는 것을 파악
    → /usr/local/hadoop/share/hadoop/yarn - hadoop 설치 후 압축 해제가 제대로 되지 않아 해당 경로에 파일 누락
    → 정상적으로 압축 해제된 파일을 해당 경로에 추가해서 해결

# 6. Hive 실행

- utilnode(master03)에서 실행 확인

```bash
ubuntu@team06-master03:~$ ssh master03
ubuntu@team06-master03:~$ hive
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.3.2/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Hive Session ID = 287e0e49-08dd-4430-a0f2-5a0ac6b85184

Logging initialized using configuration in file:/usr/local/apache-hive-3.1.2-bin/conf/hive-log4j2.properties Async: true
Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.
hive>
```

- hiveserver2 background 실행
- 로그가 남지 않도록 nohup가 수행한 명령을 /dev/null로 보냄

```bash
ubuntu@team06-master03:~$ nohup $HIVE_HOME/bin/hiveserver2 & > /dev/null
[1] 4716
ubuntu@team06-master03:~$ nohup: ignoring input and appending output to 'nohup.out'
jobs
[1]+  Running                 nohup $HIVE_HOME/bin/hiveserver2 &
```

+background 프로세스 종료 : grep 명령어를 사용해서 pid를 확인한 후 kill

```bash
ubuntu@team06-master03:~$ ps -ef | grep hiveserver
ubuntu      4866    4573 30 02:59 pts/0    00:00:19 /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Dproc_jar -Djava.library.path=/usr/local/hadoop/lib/native -Djava.library.path=/usr/local/hadoop/lib/native -Dproc_hiveserver2 -Dlog4j.configurationFile=hive-log4j2.properties -Djava.util.logging.config.file=/usr/local/hive/conf/parquet-logging.properties -Djline.terminal=jline.UnsupportedTerminal -Dyarn.log.dir=/usr/local/hadoop/logs -Dyarn.log.file=hadoop.log -Dyarn.home.dir=/usr/local/hadoop -Dyarn.root.logger=INFO,console -Xmx256m -Dhadoop.log.dir=/usr/local/hadoop/logs -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/local/hadoop -Dhadoop.id.str=ubuntu -Dhadoop.root.logger=INFO,console -Dhadoop.policy.file=hadoop-policy.xml -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /usr/local/hive/lib/hive-service-3.1.2.jar org.apache.hive.service.server.HiveServer2
ubuntu      4986    4573  0 03:00 pts/0    00:00:00 grep --color=auto hiveserver
ubuntu@team06-master03:~$ kill -9 4866
ubuntu@team06-master03:~$ jobs
[1]+  Killed                  nohup $HIVE_HOME/bin/hiveserver2
```

- beeline 접속 - 10.13 12:07 오류로 접속되지 않음.

```bash
ubuntu@team06-master03:~$ $HIVE_HOME/bin/beeline -u jdbc:hive2://localhost:10000
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.3.2/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://localhost:10000
22/10/13 03:08:22 [main]: WARN jdbc.HiveConnection: Failed to connect to localhost:10000
Could not open connection to the HS2 server. Please check the server URI and if the URI is correct, then ask the administrator to check the server status.
Error: Could not open client transport with JDBC Uri: jdbc:hive2://localhost:10000: java.net.ConnectException: Connection refused (Connection refused) (state=08S01,code=0)
Beeline version 3.1.2 by Apache Hive
```

→ 모든 클러스터와 hiveserver2까지 실행한 후 hiveserver2가 올라오기까지 대기 한 후 접속 가능 (config 파일에 오류는 딱히 없었던 것으로 판단됨)

```bash
ubuntu@team06-master03:/usr/local/hive/conf$ $HIVE_HOME/bin/beeline -u jdbc:hive2://localhost:10000
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/usr/local/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/usr/local/hadoop-3.3.2/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
Connecting to jdbc:hive2://localhost:10000
Connected to: Apache Hive (version 3.1.2)
Driver: Hive JDBC (version 3.1.2)
Transaction isolation: TRANSACTION_REPEATABLE_READ
Beeline version 3.1.2 by Apache Hive
0: jdbc:hive2://localhost:10000>
```

# 7. Spark 클러스터 실행

- master03에서 실행 (workers : master03, slave01, slave02)

```bash
# EC2 Ubuntu terminal(master03)
ubuntu@team06-master03:~$ $SPARK_HOME/sbin/start-all.sh
starting org.apache.spark.deploy.master.Master, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.master.Master-1-team06-master03.hadoop.com.out
master03: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-team06-master03.hadoop.com.out
slave01: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-team06-slave01.hadoop.com.out
slave02: starting org.apache.spark.deploy.worker.Worker, logging to /usr/local/spark/logs/spark-ubuntu-org.apache.spark.deploy.worker.Worker-1-team06-slave02.hadoop.com.out
```

- pyspark 실행

```bash
ubuntu@team06-master03:~$ /usr/local/spark/bin/pyspark
Python 3.8.10 (default, Jun 22 2022, 20:18:18)
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
Warning: Ignoring non-Spark config property: "
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
2022-10-13 03:11:20,852 WARN conf.HiveConf: HiveConf of name hive.metastore.schema.verfication does not exist
2022-10-13 03:11:20,971 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-10-13 03:11:23,560 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.1
      /_/

Using Python version 3.8.10 (default, Jun 22 2022 20:18:18)
Spark context Web UI available at http://master03:4040
Spark context available as 'sc' (master = yarn, app id = application_1665628591243_0001).
SparkSession available as 'spark'.
>>>
```

- jupyter notebook (pyspark 실행 시 자동 실행)

```jsx
ubuntu@team06-master03:~/pyspark$ pyspark
[I 16:53:11.774 NotebookApp] Serving notebooks from local directory: /home/ubuntu/pyspark
[I 16:53:11.774 NotebookApp] Jupyter Notebook 6.5.1 is running at:
[I 16:53:11.774 NotebookApp] http://172.31.40.7:8911/
[I 16:53:11.774 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
```

→ {master03 public ip}://8911로 jupyter notebook 접속
password : team06

# Appendix

- master01 서버의 ~/sh/cluster/jps-check-all.sh : 모든 node의 jps 확인

```bash
ubuntu@team06-master01:~/sh/cluster$ cat jps-check-all.sh
# Jps chaek
parallel-ssh -h ~/.ssh/all -i "sudo jps"
```

- 모든 클러스터 실행 후 최종 jps

```bash
ubuntu@team06-master01:~$ sh ~/sh/cluster/jps-check-all.sh
[1] 03:13:16 [SUCCESS] slave02
2882 Worker
2456 NodeManager
3199 Jps
2191 DataNode
[2] 03:13:16 [SUCCESS] slave01
2625 NodeManager
2359 DataNode
3288 Jps
2972 Worker
[3] 03:13:16 [SUCCESS] master03
4289 NodeManager
6483 Jps
4020 JournalNode
3862 DataNode
6170 Worker
6011 Master
4990 RunJar
2654 QuorumPeerMain
[4] 03:13:16 [SUCCESS] master01
6261 Jps
4680 NameNode
5624 ResourceManager
2505 QuorumPeerMain
5177 DFSZKFailoverController
4939 JournalNode
[5] 03:13:16 [SUCCESS] master02
3936 JournalNode
4389 ResourceManager
3797 NameNode
4117 DFSZKFailoverController
5382 Jps
2287 QuorumPeerMain
```

- spark와 hive를 제외한 모든 클러스터 실행(중지) 쉘 스크립트 작성
경로 : master01:~/sh/cluster
    - cluster-start-all.sh
    
    ```bash
    # Zookeeper
    echo " ---------------- Zookeeper -----------------"
    parallel-ssh -h ~/.ssh/master -i "/usr/local/zookeeper/bin/zkServer.sh start"
    
    # Hadoop
    echo " ------------------ Hadoop ------------------"
    start-dfs.sh
    
    # Yarn
    echo " ------------------- yarn -------------------"
    start-yarn.sh
    
    # Jps check
    echo " -------------------- jps -------------------"
    sh ~/sh/cluster/jps-check-all.sh
    ```
    
    - cluster-stop-all.sh
    
    ```bash
    # Zookeeper
    echo " ---------------- Zookeeper -----------------"
    parallel-ssh -h ~/.ssh/master -i "/usr/local/zookeeper/bin/zkServer.sh stop"
    
    # Hadoop
    echo " ------------------ Hadoop ------------------"
    stop-dfs.sh
    
    # Yarn
    echo " ------------------- yarn -------------------"
    #echo "Stopping nodemanager"
    #parallel-ssh -h ~/.ssh/worker "yarn --daemon stop nodemanager"
    #echo "Stopping resourcemanager [master01 master02]"
    #parallel-ssh -h ~/.ssh/master "yarn --daemon stop resourcemanager"
    yarn-stop.sh
    
    # Jps check
    echo " -------------------- jps -------------------"
    sh ~/sh/cluster/jps-check-all.sh
    ```
    

# Pyspark를 활용한 데이터 전처리 과정 - issue

- raw data를  모델이 학습 가능한 상태의 데이터로 처리
- 처리된 데이터를 HDFS에 저장한 후 feture store인 Amazon S3에 dvc를 이용한 전송 기능 구현

- 데이터 파이프라인 구축에 집중하기 위해 간단한 EDA 후 전처리 진행

![Untitled (1)](https://user-images.githubusercontent.com/102719063/203524914-c56c948b-28c7-46da-bcd4-97fee43cd8a1.png)

- EDA 결과에 따른 전처리 모듈 코드 작성
    - def column_selection
    - 사용할 컬럼 추출
    - def rm_missingValue
    - 결측치 제거
    - def rm_outlier
    - 이상치 제거
    - def datetime_convert
    - datetime 을 요일, 시간, 분으로 변환해서 새로운 컬럼으로 추가
    - def datetime_diff
    - pikup datetime과 dropoff datetime의 차이를 구해서 택시를 이용한 시간을 나타내는 새로운 컬럼으로 추가
    - def drop_columns
    - 학습에 용이하지 않은 datetime 컬럼 삭제

- 전처리된 spark 데이터프레임을 hdfs에 parquet 형식으로 저장

![Untitled](https://user-images.githubusercontent.com/102719063/203524936-bcea7dba-901d-49db-b058-2674dd139ecf.png)

- 주요 문제 및 해결 방안
    - 2017년도의 전월 파일을 학습시킨 모델을 만들기 위해 dataframe을 합친 후 hdfs에 파일로 저장하려 했으나 분리된 parquet 파일로 저장됨.
    → parquet 파일을 하나로 repartition 하는 옵션을 추가한 코드를 통해 하나의 parquet 파일로 저장 완료
